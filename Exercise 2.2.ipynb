{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4bc8791-2071-4457-91cc-8553b2d89335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Step 1: Load your unscaled weather observations and \"pleasant weather\" data\n",
    "path = '/Users/andrewfearney27/Downloads/'\n",
    "output_dir = os.path.join(path, '01 Data')\n",
    "cleaned_data_path = os.path.join(output_dir, 'climate_nodate_cleaned.csv')\n",
    "\n",
    "#Load data\n",
    "weather_unscaled_path = os.path.join(path, 'Dataset-weather-prediction-dataset-scaled.csv')\n",
    "pleasant_weather_path = os.path.join(path, 'Dataset-Answers-Weather_Prediction_Pleasant_Weather.csv')\n",
    "\n",
    "weather_data = pd.read_csv(weather_unscaled_path)\n",
    "pleasant_weather = pd.read_csv(pleasant_weather_path)\n",
    "\n",
    "#Convert all columns to lowercase and strip leading/trailing spaces to avoid errors\n",
    "weather_data.columns = weather_data.columns.str.lower().str.strip()\n",
    "\n",
    "# Step 2: Clean the data\n",
    "# Define columns to drop\n",
    "columns_to_drop = ['date', 'month', 'gdansk_cloud_cover', 'gdansk_humidity', 'gdansk_precipitation', 'gdansk_snow_depth',\n",
    "                   'roma_cloud_cover', 'roma_wind_speed', 'tours_pressure', 'tours_humidity', 'tours_snow_depth']\n",
    "\n",
    "#Drop columns present in the dataset \n",
    "columns_to_drop = [col for col in columns_to_drop if col in weather_data.columns]\n",
    "weather_cleaned = weather_data.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "#Drop any remaining snow depth columns if present\n",
    "weather_cleaned = weather_cleaned.loc[:, ~weather_cleaned.columns.str.endswith('_snow_depth')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3a0a7d2-6dc1-4733-afb5-fa868360b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Handle missing data\n",
    "# Fill missing data from nearby stations, using stations like Ljubljana, Sonnblick, and Oslo as substitutes\n",
    "if 'kassel_wind_speed' in weather_cleaned.columns and 'ljubljana_cloud_cover' in weather_cleaned.columns:\n",
    "    position1 = weather_cleaned.columns.get_loc('kassel_wind_speed') - 1\n",
    "    weather_cleaned.insert(position1, 'kassel_cloud_cover', weather_cleaned['ljubljana_cloud_cover'])\n",
    "\n",
    "if 'munchenb_humidity' in weather_cleaned.columns and 'sonnblick_wind_speed' in weather_cleaned.columns:\n",
    "    position2 = weather_cleaned.columns.get_loc('munchenb_humidity') - 1\n",
    "    weather_cleaned.insert(position2, 'munchenb_wind_speed', weather_cleaned['sonnblick_wind_speed'])\n",
    "\n",
    "if 'stockholm_pressure' in weather_cleaned.columns and 'oslo_humidity' in weather_cleaned.columns:\n",
    "    position3 = weather_cleaned.columns.get_loc('stockholm_pressure') - 1\n",
    "    weather_cleaned.insert(position3, 'stockholm_humidity', weather_cleaned['oslo_humidity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af0e7589-e085-4406-ae21-3f43d16c54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Export the cleaned data\n",
    "weather_cleaned.to_csv(cleaned_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "708be703-2023-42e7-872a-255626f28f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Create 'X' and 'y' matrix\n",
    "#Ensure columns are cleaned again\n",
    "weather_cleaned.columns = weather_cleaned.columns.str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "db88c0c1-258d-46c7-a791-4bdcd1ec156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'date' column safely if it exists in pleasant_weather\n",
    "pleasant_weather.columns = pleasant_weather.columns.str.lower().str.strip()\n",
    "if 'date' in pleasant_weather.columns:\n",
    "    y = pleasant_weather.drop(columns=['date'], axis=1).values\n",
    "else:\n",
    "    y = pleasant_weather.values\n",
    "\n",
    "# Use cleaned weather data for X matrix\n",
    "X = weather_cleaned.values\n",
    "\n",
    "# Ensure X and y have the same length\n",
    "min_length = min(len(X), len(y))\n",
    "X = X[:min_length]\n",
    "y = y[:min_length]\n",
    "\n",
    "# Reshape X into (n_samples, 15, 9) and y into (n_samples, 15)\n",
    "X = X.reshape(-1, 15, 9)\n",
    "y = y.reshape(-1, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e79f7a5-1b70-47b7-ba87-c67f2c9c1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted X shape: (22950, 15, 9)\n",
      "Adjusted y shape: (22950, 15)\n"
     ]
    }
   ],
   "source": [
    "#Step 6\n",
    "#Ensure X and y have the same length\n",
    "min_length = min(len(X), len(y))\n",
    "X = X[:min_length]\n",
    "y = y[:min_length]\n",
    "\n",
    "print(f\"Adjusted X shape: {X.shape}\")\n",
    "print(f\"Adjusted y shape: {y.shape}\")\n",
    "\n",
    "# Reshape X into (n_samples, 15, 9) and y into (n_samples, 15)\n",
    "X = X.reshape(-1, 15, 9)\n",
    "y = y.reshape(-1, 15)\n",
    "\n",
    "# Now proceed with splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7af83bbc-954c-4a24-9c74-006b8027145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Step 7: Build the CNN model\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "n_hidden = 2\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = len(y_train[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(n_hidden, kernel_size=2, activation='relu', input_shape=(timesteps, input_dim)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab34788e-5719-46a9-9c67-17686d34ae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 330us/step - accuracy: 0.0975 - loss: 11.7364 - val_accuracy: 0.1028 - val_loss: 13.2078\n",
      "Epoch 2/5\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 0.0919 - loss: 18.2314 - val_accuracy: 0.0865 - val_loss: 17.1887\n",
      "Epoch 3/5\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278us/step - accuracy: 0.0769 - loss: 20.9504 - val_accuracy: 0.0776 - val_loss: 21.9587\n",
      "Epoch 4/5\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - accuracy: 0.0714 - loss: 23.8693 - val_accuracy: 0.0741 - val_loss: 27.3477\n",
      "Epoch 5/5\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279us/step - accuracy: 0.0792 - loss: 29.1329 - val_accuracy: 0.0784 - val_loss: 34.2420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31fb87d50>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 8: Train the model\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test, y_test), epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "31e2e50a-d5af-4200-946e-d54b8e4f2b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264us/step\n",
      "Pred        basel  belgrade  madrid\n",
      "True                               \n",
      "basel        2220         5     730\n",
      "belgrade      650         5     224\n",
      "budapest      126         0      36\n",
      "debilt         49         1      14\n",
      "dusseldorf     18         0       7\n",
      "heathrow       49         0      18\n",
      "kassel          7         0       2\n",
      "ljubljana      36         0      10\n",
      "maastricht      6         0       1\n",
      "madrid        270         1      89\n",
      "munchenb        6         0       2\n",
      "oslo            2         0       2\n",
      "stockholm       3         0       0\n",
      "valentia        1         0       0\n"
     ]
    }
   ],
   "source": [
    "#Step 9: Create confusion matrix\n",
    "predictions = {0: 'basel', 1: 'belgrade', 2: 'budapest', 3: 'debilt', 4: 'dusseldorf', 5: 'heathrow', \n",
    "               6: 'kassel', 7: 'ljubljana', 8: 'maastricht', 9: 'madrid', 10: 'munchenb', 11: 'oslo', \n",
    "               12: 'sonnblink', 13: 'stockholm', 14: 'valentia'}\n",
    "\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([predictions[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([predictions[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
